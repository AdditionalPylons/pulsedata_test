{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "d84bacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "22a98ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import pathlib\n",
    "\n",
    "from pyspark.sql import (\n",
    "                        DataFrame,\n",
    "                        functions as sf,\n",
    "                        SparkSession,\n",
    "                        types,\n",
    "                        Window\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "7172fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputMode = Enum(\"OutputMode\", \"results_only stats verbose\")\n",
    "ReadMode = Enum(\"ReadMode\", \"csv_only parquet_only csv_to_parquet\")\n",
    "\n",
    "# Config constants, changing these\n",
    "# will change the flow, runtime, and\n",
    "# output of the notebook\n",
    "DATA_FOLDER = \"data_test\"\n",
    "OUTPUT_MODE = OutputMode.results_only\n",
    "READ_MODE = ReadMode.parquet_only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "9ca2fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "12f97337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limited number of schemas with few columns,\n",
    "# so let's define them here. Makes data types more\n",
    "# reliable and speeds up DF creation, since we skip\n",
    "# initial pass through the file to infer schema.\n",
    "\n",
    "CODE_GROUPS_SCHEMA = types.StructType([\n",
    "    types.StructField(\"ICD_CODE\",types.StringType(),nullable=True),\n",
    "    types.StructField(\"GROUP\",types.StringType(),nullable=True),\n",
    "])\n",
    "\n",
    "DEMOGRAPHICS_SCHEMA = types.StructType([\n",
    "    types.StructField(\"CLINIC\",types.StringType(),nullable=True),\n",
    "    types.StructField(\"MRN\",types.StringType(),nullable=True),\n",
    "    types.StructField(\"FIRST_NAME\",types.StringType(),nullable=True),\n",
    "    types.StructField(\"LAST_NAME\",types.StringType(),nullable=True),\n",
    "    types.StructField(\"DATE_OF_BIRTH\",types.DateType(),nullable=True),\n",
    "])\n",
    "\n",
    "ENCOUNTERS_SCHEMA = types.StructType([\n",
    "    types.StructField(\"DATE\",types.DateType(),nullable=True),\n",
    "    types.StructField(\"ENC_ID\",types.StringType(),nullable=True),\n",
    "    types.StructField(\"MRN\",types.StringType(),nullable=True),\n",
    "    types.StructField(\"ICD_CODE\",types.StringType(),nullable=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "697cdcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_parquet(path: str) -> None:\n",
    "    \"\"\"\n",
    "    Export csv file as a parquet file\n",
    "    with the appropriate schema.\n",
    "\n",
    "    :param path: File path to the csv\n",
    "    \"\"\"\n",
    "\n",
    "    filename = path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    if filename.casefold() == \"code_groups\":\n",
    "        schema = CODE_GROUPS_SCHEMA\n",
    "    elif filename.casefold() == \"demographics\":\n",
    "        schema = DEMOGRAPHICS_SCHEMA\n",
    "    elif filename.casefold() == \"encounters\":\n",
    "        schema = ENCOUNTERS_SCHEMA\n",
    "\n",
    "    df = (\n",
    "        spark.read\n",
    "        .option(\"delimiter\", \"|\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .schema(schema)\n",
    "        .csv(path)\n",
    "    )\n",
    "            \n",
    "    parquet_path = path.split(\".\")[0]+\".parquet\"\n",
    "\n",
    "    df.write.mode(\"overwrite\").parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "81c3f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read file and convert to a Spark DataFrame.\n",
    "    Supported filetypes are csv & parquet.\n",
    "\n",
    "    :param path: File path of the file to process\n",
    "    \"\"\"\n",
    "\n",
    "    filename, extension = path.split(\"/\")[-1].split(\".\")\n",
    "\n",
    "    if filename.casefold() == \"code_groups\":\n",
    "        schema = CODE_GROUPS_SCHEMA\n",
    "    elif filename.casefold() == \"demographics\":\n",
    "        schema = DEMOGRAPHICS_SCHEMA\n",
    "    elif filename.casefold() == \"encounters\":\n",
    "        schema = ENCOUNTERS_SCHEMA\n",
    "\n",
    "    if extension == \"csv\":\n",
    "        df = (\n",
    "            spark.read\n",
    "            .option(\"delimiter\", \"|\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .schema(schema)\n",
    "            .csv(path)\n",
    "        )\n",
    "\n",
    "    elif extension == \"parquet\":\n",
    "         df = (\n",
    "            spark.read\n",
    "            .schema(schema)\n",
    "            .parquet(path)\n",
    "         )\n",
    "    else:\n",
    "        raise TypeError(f\"{path} is not a csv or parquet file!\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "e2ade5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(sdf: DataFrame, name_map: dict) -> DataFrame:\n",
    "    for from_col, to_col in name_map.items():\n",
    "        sdf = sdf.withColumnRenamed(from_col, to_col)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "50852b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_bucket(age: int) -> int:\n",
    "    if age > 100:\n",
    "        return 100\n",
    "    elif age > 90:\n",
    "        return 90\n",
    "    elif age > 80:\n",
    "        return 80\n",
    "    elif age > 70:\n",
    "        return 70\n",
    "    elif age > 60:\n",
    "        return 60\n",
    "    elif age > 50:\n",
    "        return 50\n",
    "    elif age > 40:\n",
    "        return 40\n",
    "    else:\n",
    "        # ignore all patients under 40\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "339af312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make any changes in constants above, not here\n",
    "data_folder = pathlib.Path(DATA_FOLDER)\n",
    "\n",
    "# spark.read requires path to be a str,\n",
    "# converting them all here in one place\n",
    "# instead of handling inside funcs\n",
    "if READ_MODE == ReadMode.csv_only or READ_MODE == ReadMode.csv_to_parquet:\n",
    "    code_groups = str(pathlib.Path('code_groups.csv'))\n",
    "    demographics = str(data_folder.joinpath('demographics.csv'))\n",
    "    encounters = str(data_folder.joinpath('encounters.csv'))\n",
    "\n",
    "if READ_MODE == ReadMode.csv_to_parquet:\n",
    "    for csv in [code_groups, demographics, encounters]:\n",
    "        csv_to_parquet(csv)\n",
    "\n",
    "if READ_MODE == ReadMode.parquet_only or READ_MODE == ReadMode.csv_to_parquet:\n",
    "    code_groups = str(pathlib.Path('code_groups.parquet'))\n",
    "    demographics = str(data_folder.joinpath('demographics.parquet'))\n",
    "    encounters = str(data_folder.joinpath('encounters.parquet'))\n",
    "\n",
    "code_groups_sdf = read_file(code_groups)\n",
    "demo_sdf = read_file(demographics)\n",
    "enc_sdf = read_file(encounters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "1fe1b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "\n",
    "def check_dupes(df: DataFrame) -> None:\n",
    "    return (\n",
    "        df.groupBy(df.columns)\n",
    "        .count()\n",
    "        .where(sf.col(\"count\") > 1)\n",
    "        .select(sf.coalesce(sf.sum(\"count\"), sf.lit(0)))\n",
    "        .withColumnRenamed(\"coalesce(sum(count), 0)\", f\"Duplicate Count\")\n",
    "        .show()\n",
    "    )\n",
    "\n",
    "def check_nulls(df: DataFrame) -> None:\n",
    "    print(\"Null Counts\")\n",
    "    return df.select([sf.count(sf.when(sf.col(c).isNull(), c)).alias(c) for c in df.columns]\n",
    "   ).show()\n",
    "\n",
    "def run_single_df_checks(df: DataFrame) -> None:\n",
    "    check_dupes(df)\n",
    "    check_nulls(df)\n",
    "    df.describe().show()\n",
    "\n",
    "def run_all_checks():\n",
    "    print(\"-----CODE GROUPS-----\")\n",
    "    run_single_df_checks(code_groups_sdf)\n",
    "    print(\"-----DEMO-----\")\n",
    "    run_single_df_checks(demo_sdf)\n",
    "    print(\"-----ENC-----\")\n",
    "    run_single_df_checks(enc_sdf)\n",
    "\n",
    "if OUTPUT_MODE != OutputMode.results_only:\n",
    "    run_all_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "597f8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResourceType = Enum(\"ResourceType\", \"code_groups demographics encounters\")\n",
    "\n",
    "def clean_data(df: DataFrame, type: ResourceType) -> DataFrame:\n",
    "    # Pulled from https://www.johndcook.com/blog/2019/05/05/regex_icd_codes/\n",
    "    N = \"\\d{3}\\.?\\d{0,2}\"\n",
    "    E = \"E\\d{3}\\.?\\d?\"\n",
    "    V = \"V\\d{2}\\.?\\d{0,2}\"\n",
    "    icd9_regex = \"|\".join([N, E, V])\n",
    "    icd10_regex = \"[A-TV-Z][0-9][0-9AB]\\.?[0-9A-TV-Z]{0,4}\"\n",
    "\n",
    "    # Allows weird dates like 0000-99-99 but good enough for our purposes\n",
    "    date_regex = \"[0-9]{4}-[0-9]{2}-[0-9]{2}\"\n",
    "    #name_regex = r\"[A-Za-z]+\"\n",
    "\n",
    "    if type == ResourceType.code_groups:\n",
    "        df = (df.where(\n",
    "            (sf.col(\"ICD_CODE\").isNotNull())\n",
    "        &   (sf.col(\"GROUP\").isNotNull())\n",
    "        &   (sf.col(\"ICD_CODE\").rlike(icd9_regex) | sf.col(\"ICD_CODE\").rlike(icd10_regex))\n",
    "        ))\n",
    "    elif type == ResourceType.demographics:\n",
    "        df = (df.where(\n",
    "            (sf.col(\"DATE_OF_BIRTH\").isNotNull())\n",
    "        &   (sf.col(\"MRN\").isNotNull())\n",
    "        &   (sf.col(\"DATE_OF_BIRTH\").rlike(date_regex))\n",
    "        &   (sf.col(\"DATE_OF_BIRTH\") > sf.date_sub(sf.current_date(), (120 * 365)))\n",
    "        #&  (sf.col(\"FIRST_NAME\").rlike(name_regex))\n",
    "        #&  (sf.col(\"LAST_NAME\").rlike(name_regex))\n",
    "        ))\n",
    "    elif type == ResourceType.encounters:\n",
    "        df = (df.where(\n",
    "            (sf.col(\"DATE\").isNotNull())\n",
    "        &   (sf.col(\"MRN\").isNotNull())\n",
    "        &   (sf.col(\"ICD_CODE\").isNotNull())\n",
    "        &   (sf.col(\"DATE\").rlike(date_regex))\n",
    "        &   (sf.col(\"DATE\") > sf.date_sub(sf.current_date(), (15 * 365)))\n",
    "        &   (sf.col(\"ICD_CODE\").rlike(icd9_regex) | sf.col(\"ICD_CODE\").rlike(icd10_regex))\n",
    "        ))\n",
    "\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "2f0572e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_groups_sdf = clean_data(code_groups_sdf, ResourceType.code_groups)\n",
    "demo_sdf = clean_data(demo_sdf, ResourceType.demographics)\n",
    "enc_sdf = clean_data(enc_sdf, ResourceType.encounters)\n",
    "\n",
    "if OUTPUT_MODE != OutputMode.results_only:\n",
    "    run_all_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "153070a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_encounters(enc_sdf):\n",
    "    sdf2 = (\n",
    "        enc_sdf\n",
    "        .withColumn('ROW_ID', sf.monotonically_increasing_id())\n",
    "        .withColumn('PRIORITY', sf.row_number().over(Window.partitionBy('ENC_ID').orderBy('ROW_ID')))\n",
    "        .drop('ROW_ID')\n",
    "        .groupBy(['ENC_ID', 'DATE', 'MRN'])\n",
    "        .pivot('PRIORITY')\n",
    "        .agg(sf.first('ICD_CODE'))\n",
    "    )\n",
    "    return rename_columns(sdf2, {i: f'ICD_CODE_{i}' for i in sdf2.columns[3:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "7e3d0150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "enc_sdf_transformed = transform_encounters(enc_sdf)\n",
    "\n",
    "if OUTPUT_MODE == OutputMode.verbose:\n",
    "    enc_sdf.show()\n",
    "    enc_sdf_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "c2a85008",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_sdf_mrn = demo_sdf.withColumn(\"FULL_MRN\", sf.concat(\"CLINIC\", \"MRN\"))\n",
    "\n",
    "if OUTPUT_MODE == OutputMode.verbose:\n",
    "    demo_sdf.show()\n",
    "    demo_sdf_mrn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "48aa1e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sdf_added_cols = (\n",
    "    enc_sdf_transformed\n",
    "    .join(demo_sdf_mrn, enc_sdf_transformed.MRN == demo_sdf_mrn.FULL_MRN)\n",
    "    .withColumn('AGE', sf.datediff('DATE', 'DATE_OF_BIRTH') / 365)\n",
    "    .withColumn('AGE_BUCKET', sf.udf(age_bucket)(sf.col('AGE')))\n",
    "    .withColumn('ICD_CODE', sf.expr(\"stack(4, ICD_CODE_1, ICD_CODE_2, ICD_CODE_3, ICD_CODE_4)\"))\n",
    ")\n",
    "\n",
    "joined = (\n",
    "    enc_sdf_added_cols\n",
    "    .join(code_groups_sdf, enc_sdf_added_cols.ICD_CODE==code_groups_sdf.ICD_CODE)\n",
    ")\n",
    "\n",
    "if OUTPUT_MODE == OutputMode.verbose:\n",
    "    joined.show()\n",
    "\n",
    "results = (\n",
    "    joined\n",
    "    .groupBy('AGE_BUCKET', 'GROUP').count()\n",
    "    .groupBy('AGE_BUCKET', 'count').agg(sf.collect_set('GROUP'))\n",
    "    .withColumnRenamed('collect_set(GROUP)', 'GROUPS')\n",
    "    .withColumn(\n",
    "        '_row',\n",
    "        sf.row_number().over(Window().partitionBy(['AGE_BUCKET']).orderBy(sf.desc('count'))))\n",
    "    .filter(sf.col('_row') == 1)\n",
    "    .drop('_row')\n",
    "    .filter(sf.col('AGE_BUCKET') != 0)\n",
    "    .orderBy('AGE_BUCKET')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "e762006b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1405:=====================================>              (146 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---------+\n",
      "|AGE_BUCKET|count|   GROUPS|\n",
      "+----------+-----+---------+\n",
      "|        50|    1|[1, 2, 7]|\n",
      "|        70|    1|   [2, 9]|\n",
      "+----------+-----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f479aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
